{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979155a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.subplots as sp\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import json\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import tensorflow as tfs\n",
    "\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc9605",
   "metadata": {},
   "source": [
    "# weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\samr0\\OneDrive - KU Leuven\\Documents\\!School\\master\\Thesis\\data\\aws_10min.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, index_col='timestamp', parse_dates=True)\n",
    "cutoff_timestamp = \"2022-06-19 04:20:00\"\n",
    "\n",
    "df = df.loc[:cutoff_timestamp]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf951c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\samr0\\OneDrive - KU Leuven\\Documents\\!School\\master\\Thesis\\data\\aws_10min_rest.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file_path, index_col='timestamp', parse_dates=True)\n",
    "cutoff_timestamp = \"2022-06-19 04:30:00\"\n",
    "\n",
    "df2 = df2.loc[cutoff_timestamp:]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df, df2])\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diffs = df_combined.index.to_series().diff().dropna()\n",
    "print(time_diffs.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9469e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#haversine formula to compute the great-circle distance between two points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c  #distance in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d887671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined['code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_stations = df_combined.drop_duplicates(subset=\"code\", keep=\"first\")\n",
    "df_unique_stations[['lat', 'lon']] = df_unique_stations['the_geom'].str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)').astype(float)\n",
    "\n",
    "df_unique_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4799d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the households file with lats and longs + only the ids that are in the database\n",
    "#json files\n",
    "file_path = r\"C:\\Users\\samr0\\OneDrive - KU Leuven\\Documents\\!School\\master\\Thesis\\data\\households_in_database.json\"\n",
    "#read JSON into a dataFrame\n",
    "df_households = pd.read_json(file_path)\n",
    "\n",
    "df_households.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "household_id = \"7847f5f7\"\n",
    "row = df_households[df_households[\"id\"] == household_id]\n",
    "lat = row[\"latitude\"]\n",
    "lon = row[\"longitude\"]\n",
    "\n",
    "df_unique_stations['distance_km'] = df_unique_stations.apply(lambda row: haversine(lat, lon, row['lat'], row['lon']), axis=1)\n",
    "df_unique_stations = df_unique_stations.sort_values(\"distance_km\")\n",
    "df_unique_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_unique_stations[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b07a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34837c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8de73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b210d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add the ghi values of the database\n",
    "file_path = r\"C:\\Users\\samr0\\OneDrive - KU Leuven\\Documents\\!School\\master\\Thesis\\data\\meteoStationsDatabaseData.csv\"\n",
    "\n",
    "meteoStationsData = pd.read_csv(file_path, index_col='_time', parse_dates=True)\n",
    "meteoStationsData[\"code\"] = meteoStationsData[\"nodeId\"].str[-4:].astype(int)\n",
    "meteoStationsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512c08a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_diffs = meteoStationsData.index.to_series().diff().dropna()\n",
    "print(time_diffs.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269c7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "codesInDatabase = [6434, 6438, 6455, 6459, 6464, 6472, 6477, 6484]\n",
    "#get only data of stations in the database\n",
    "df_meteo = df_combined[df_combined[\"code\"].isin(codesInDatabase)]\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo.index = pd.to_datetime(df_meteo.index)\n",
    "#reset index to merge on both timestamp and code\n",
    "meteoStationsData_reset = meteoStationsData.reset_index()\n",
    "df_meteo_reset = df_meteo.reset_index()\n",
    "\n",
    "df_meteo_reset.rename(columns={'timestamp': '_time'}, inplace=True)\n",
    "\n",
    "df_meteo_reset['_time'] = pd.to_datetime(df_meteo_reset['_time'])\n",
    "df_meteo_reset['_time'] = df_meteo_reset['_time'].dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "#merge on both timestamp and code\n",
    "merged_df = pd.merge(meteoStationsData_reset, df_meteo_reset, on=['_time', 'code'], how='inner')\n",
    "\n",
    "#set timestamp back as index\n",
    "merged_df.set_index('_time', inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5f323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c594f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7521b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_stations = merged_df.drop_duplicates(subset=\"code\", keep=\"first\")\n",
    "df_unique_stations[['lat', 'lon']] = df_unique_stations['the_geom'].str.extract(r'POINT \\(([^ ]+) ([^ ]+)\\)').astype(float)\n",
    "df_unique_stations['distance_km'] = df_unique_stations.apply(lambda row: haversine(lat, lon, row['lat'], row['lon']), axis=1)\n",
    "df_unique_stations = df_unique_stations.sort_values(\"distance_km\")\n",
    "df_unique_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ccaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb904c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe19ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e065d810",
   "metadata": {},
   "source": [
    "# if not including the ghi data\n",
    "merged_df = df_combined\n",
    "merged_df = merged_df.rename_axis('_time')\n",
    "merged_df['_time'] = pd.to_datetime(merged_df.index)\n",
    "merged_df['_time'] = merged_df['_time'].dt.tz_localize('UTC')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 closest ones and selection\n",
    "df_closest = df_unique_stations[:3]\n",
    "\n",
    "df_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23964142",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_codes = df_closest[\"code\"].unique()\n",
    "unique_codes_list = unique_codes.tolist()\n",
    "unique_codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all data from nearby stations\n",
    "df_meteo = merged_df[merged_df[\"code\"].isin(unique_codes_list)]\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo['wind_speed'] = df_meteo['wind_speed_10m'].combine_first(df_meteo['wind_speed_avg_30m'])\n",
    "\n",
    "df_meteo = df_meteo.drop(columns = [\"FID\", \"the_geom\", \"temp_grass_pt100_avg\", \"temp_soil_avg_5cm\",\n",
    "                                            \"temp_soil_avg_10cm\", \"temp_soil_avg_20cm\", \"temp_soil_avg_50cm\",\n",
    "                                            \"qc_flags\", \"wind_speed_10m\", \"wind_speed_avg_30m\"])\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26947ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lat, lon and distance_km\n",
    "df_selection = df_closest[['code', 'lat', 'lon', 'distance_km']]\n",
    "\n",
    "df_meteo = df_meteo.reset_index()\n",
    "\n",
    "df_meteo = pd.merge(df_meteo, df_selection, on=\"code\", how=\"left\")\n",
    "\n",
    "df_meteo = df_meteo.set_index(\"_time\")\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc08c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average weather taking distance into account\n",
    "\n",
    "#define a function to calculate the weighted average for a given column\n",
    "def weighted_average(group, weight_column='distance_km'):\n",
    "    #calculate the weights as the inverse of distance (closer stations get higher weight)\n",
    "    weights = 1 / group[weight_column]\n",
    "    \n",
    "    #compute the weighted average for each column in the group\n",
    "    return (group.drop(columns=[weight_column]).multiply(weights, axis=0)).sum() / weights.sum()\n",
    "\n",
    "#drop the nodeId, not a number\n",
    "df_meteo = df_meteo.drop(columns = [\"nodeId\"])\n",
    "\n",
    "#group by timestamp and apply the weighted average function to each group\n",
    "df_meteo_avg_weighted = df_meteo.groupby(df_meteo.index).apply(weighted_average)\n",
    "\n",
    "df_meteo_avg_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e69c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diffs = df_meteo_avg_weighted.index.to_series().diff().dropna()\n",
    "print(time_diffs.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_avg_weighted.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_resampled = df_meteo_avg_weighted.resample('15min').mean()\n",
    "df_weather_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_filled = df_meteo_avg_weighted.resample('15min').ffill()\n",
    "\n",
    "df_weather_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9001179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle NaN values\n",
    "df_weather_resampled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display rows with NaN values\n",
    "nan_rows = df_weather_resampled[df_weather_resampled.isna().any(axis=1)]\n",
    "\n",
    "#show the rows containing NaN values\n",
    "print(nan_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13f346",
   "metadata": {},
   "source": [
    "# add the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7efe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\samr0\\OneDrive - KU Leuven\\Documents\\!School\\master\\Thesis\\data\\inverter_power_data_7847f5f7_normalised_15min.csv\"\n",
    "\n",
    "df_data = pd.read_csv(file_path, index_col='_time', parse_dates=True)\n",
    "#df_data.index = df_data.index.tz_convert('Europe/Brussels')\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd9c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f8c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dac2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f38900c",
   "metadata": {},
   "source": [
    "# WITH GHI DATA WE LIMIT THE AMOUNT OF AVAILABLE DATA\n",
    "# USE LEFT JOIN FOR ALL DATA, BUT THEN NAN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c78065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data with weather\n",
    "df_data = df_data.reset_index()\n",
    "\n",
    "df_data['_time'] = pd.to_datetime(df_data['_time'])\n",
    "df_data['_time'] = df_data['_time'].dt.tz_localize('UTC')\n",
    "df_data.set_index('_time', inplace=True)\n",
    "\n",
    "\n",
    "df_data = pd.merge(df_data, df_weather_resampled, how='right', left_index=True, right_index=True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x = df_data.index\n",
    "y = df_data['normalized_value']\n",
    "\n",
    "#apply Savitzky-Golay filter\n",
    "smoothed_y = savgol_filter(y, window_length=60*4+1, polyorder=2)\n",
    "\n",
    "#store residuals\n",
    "residuals = y - smoothed_y\n",
    "\n",
    "#reconstruct the original signal\n",
    "reconstructed_y = smoothed_y + residuals\n",
    "\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=x, y=y, mode='lines', name='original Power'))\n",
    "fig2.add_trace(go.Scatter(x=x, y=smoothed_y, mode='lines', name='smoothed Power'))\n",
    "fig2.add_trace(go.Scatter(x=x, y=reconstructed_y, mode='lines', name='reconstructed Power'))\n",
    "\n",
    "#update layout for better visualization\n",
    "fig2.update_layout(\n",
    "    title='Savitzky-Golay Smoothing and Reconstruction',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Mean Actual Power (W)',\n",
    "    xaxis_rangeslider_visible=True\n",
    ")\n",
    "\n",
    "# show the plot\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra features\n",
    "\n",
    "df_data['hour'] = df_data.index.hour\n",
    "df_data['day_of_week'] = df_data.index.dayofweek\n",
    "df_data['month'] = df_data.index.month\n",
    "\n",
    "df_data['hour_sin'] = np.sin(2 * np.pi * df_data['hour'] / 24)\n",
    "df_data['hour_cos'] = np.cos(2 * np.pi * df_data['hour'] / 24)\n",
    "df_data['day_of_year'] = df_data.index.dayofyear\n",
    "df_data['day_of_year_sin'] = np.sin(2 * np.pi * df_data['day_of_year'] / 365)\n",
    "df_data['day_of_year_cos'] = np.cos(2 * np.pi * df_data['day_of_year'] / 365)\n",
    "\n",
    "df_data['minute'] = df_data.index.minute\n",
    "\n",
    "# Encode the 15-minute intervals within an hour\n",
    "df_data['minute_sin'] = np.sin(2 * np.pi * df_data['minute'] / 60)\n",
    "df_data['minute_cos'] = np.cos(2 * np.pi * df_data['minute'] / 60)\n",
    "\n",
    "\n",
    "#time_windows = ['15min', '30min', '45min']\n",
    "#for window in time_windows:\n",
    "#    df_data[f'ma_{window}'] = (\n",
    "#        df_data['normalized_value']\n",
    "#        .rolling(window=window, min_periods=1)\n",
    "#        .mean()\n",
    "#        .shift(1) #makes it lagged MA\n",
    "#    )\n",
    "\n",
    "# Drop rows with NaN values introduced by moving averages\n",
    "#df_data = df_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af97af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data[\"mean_actualPowerTot_W_inverter\"].isna().sum())\n",
    "df_data = df_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cde21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "columns_to_normalize = [\n",
    "    'precip_quantity', \n",
    "    'temp_dry_shelter_avg', \n",
    "    'temp_soil_avg',\n",
    "    'wind_direction',\n",
    "    'wind_gusts_speed', \n",
    "    'humidity_rel_shelter_avg', \n",
    "    'pressure', \n",
    "    'sun_duration', \n",
    "    'short_wave_from_sky_avg', \n",
    "    'sun_int_avg', \n",
    "    'wind_speed',\n",
    "'diffuseIrradiance_Wpm2',\n",
    "'directNormalIrradiance_Wpm2',\n",
    "'globalHorizontalIrradiance_Wpm2'\n",
    "]\n",
    "\n",
    "#apply MinMax scaling only to mean_actualPowerTot_W_inverter\n",
    "df_data[columns_to_normalize] = scaler.fit_transform(df_data[columns_to_normalize])\n",
    "df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_data.index.duplicated()\n",
    "print(\"number of duplicates: \", duplicates.sum())\n",
    "df_data = df_data[~df_data.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra shift of 24 hours ago\n",
    "df_data[\"normalized_value_shift_24\"] = df_data[['normalized_value']].shift(freq='D')\n",
    "df_data = df_data.dropna()\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24855c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_data.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix[['mean_actualPowerTot_W_inverter']].sort_values(by='mean_actualPowerTot_W_inverter', ascending=False), \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            vmin=-1, vmax=1,\n",
    "            cbar_kws={'label': 'Correlation coefficient'})\n",
    "\n",
    "plt.title(\"Correlation of features with 'mean_actualPowerTot_W_inverter'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_test = df_data\n",
    "\n",
    "\n",
    "df_corr_test = df_corr_test.drop(columns = [\"short_wave_from_sky_avg\", \"adjusted_P_max\", \"scaled_adjusted_P_max\", \"sun_int_avg\",\n",
    "                                           \"lat\", \"lon\", \"night\", \"normalized_value\", \"code\", \"minute\", \"hour\", \"day_of_year\"])\n",
    "\n",
    "df_corr_test = df_corr_test.rename(columns={\n",
    "    \"mean_actualPowerTot_W_inverter\": \"Mean actual power\",\n",
    "    \"globalHorizontalIrradiance_Wpm2\": \"Global horizontal irradiance\",\n",
    "    \"directNormalIrradiance_Wpm2\": \"Direct normal irradiance\",\n",
    "    \"diffuseIrradiance_Wpm2\": \"Diffuse irradiance\",\n",
    "    \"sun_duration\": \"Sun duration\",\n",
    "    \"temp_soil_avg\": \"Temp soil avg\",\n",
    "    \"temp_dry_shelter_avg\": \"Temp dry shelter avg\",\n",
    "    \"day_of_year_sin\": \"Day of year sin\",\n",
    "    \"wind_gusts_speed\": \"Wind gusts speed\",\n",
    "    \"wind_speed\": \"Wind speed\",\n",
    "    \"hour_sin\": \"Hour sin\",\n",
    "    \"normalized_value_shift_24\": \"Normalized value shift 24\",\n",
    "    \"day_of_week\": \"Day of week\",\n",
    "    \"minute_sin\": \"Minute sin\",\n",
    "    \"minute_cos\": \"Minute cos\",\n",
    "    \"precip_quantity\": \"Precip quantity\",\n",
    "    \"wind_direction\": \"Wind direction\",\n",
    "    \"day_of_year_cos\": \"Day of year cos\",\n",
    "    \"hour_cos\": \"Hour cos\",\n",
    "    \"humidity_rel_shelter_avg\": \"Humidity rel shelter avg\"\n",
    "})\n",
    "\n",
    "\n",
    "correlation_matrix = df_corr_test.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix[['Mean actual power']].sort_values(by='Mean actual power', ascending=False), \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            vmin=-1, vmax=1,\n",
    "            cbar_kws={'label': 'Correlation coefficient'})\n",
    "\n",
    "plt.title(\"Correlation of features with 'Mean actual power'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e44c26",
   "metadata": {},
   "source": [
    "# train - cv - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583561cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "input_steps = 4*8\n",
    "output_steps = 1    #predict one timestep ahead\n",
    "step_size_train = 1\n",
    "step_size_test = 1      #shift for testing\n",
    "step_size_val = 1\n",
    "\n",
    "#select relevant columns for features and target\n",
    "features = [\n",
    "    \"normalized_value\",\n",
    "    #\"mean_actualPowerTot_W_inverter_scaled\",\n",
    "     #\"day_of_week\", \"month\", \n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    #\"day_of_year_sin\", \"day_of_year_cos\",\n",
    "    \"minute_sin\", \"minute_cos\",\n",
    "    \"temp_dry_shelter_avg\",\n",
    "    \"normalized_value_shift_24\",\n",
    "    'diffuseIrradiance_Wpm2',\n",
    "    'directNormalIrradiance_Wpm2',\n",
    "    'globalHorizontalIrradiance_Wpm2'\n",
    "]\n",
    "\n",
    "target = \"normalized_value\"\n",
    "#target = \"mean_actualPowerTot_W_inverter_scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea254b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_window(data, input_steps=1440, output_steps=1, feature_columns=None, target_column=None, step_size=1):\n",
    "    \"\"\"\n",
    "    Generate sliding windows for a given range of data.\n",
    "    - input_steps: Number of timesteps in the input window.\n",
    "    - output_steps: Number of timesteps to predict.\n",
    "    - feature_columns: List of feature column names.\n",
    "    - target_column: Name of the target column.\n",
    "    - step_size: Shift between consecutive windows.\n",
    "    \"\"\"\n",
    "    X, y, X_indices, y_indices = [], [], [], []\n",
    "    for i in range(0, len(data) - input_steps - output_steps + 1, step_size):\n",
    "        # input: feature columns over the input window\n",
    "        X_window = data[feature_columns].iloc[i:i+input_steps]\n",
    "        X.append(X_window)\n",
    "        X_indices.append(data.index[i:i+input_steps])  #store corresponding indices\n",
    "        \n",
    "        # target: target column for the output window\n",
    "        y_window = data[target_column].iloc[i+input_steps:i+input_steps+output_steps]\n",
    "        y.append(y_window)\n",
    "        y_indices.append(data.index[i+input_steps:i+input_steps+output_steps])  #store corresponding indices\n",
    "    \n",
    "    print(\"Generated sliding windows - X.size:\", len(X), \"y.size:\", len(y))\n",
    "    return X, y, X_indices, y_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf626e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train test sizes\n",
    "train_size = 20 * 24 * 4  # 20 days in 15 minutes\n",
    "val_size = 5 * 24 * 4    # 5 days in 15 minutes\n",
    "test_size = 5 * 24 * 4    # 5 days in 15 minutes\n",
    "\n",
    "# generate train-test splits dynamically, jump by test_size forward between splits\n",
    "splits = []\n",
    "for i in range(0, len(df_data) - train_size - test_size + 1, train_size + val_size + test_size):#if all splits are used for training, there can't be\n",
    "    #overlap, so jump train_size + test_size\n",
    "    train_data = df_data.iloc[i:i+train_size]\n",
    "    val_data = df_data.iloc[i+train_size:i+train_size+val_size]\n",
    "    test_data = df_data.iloc[i+train_size+val_size:i+train_size+val_size+test_size]\n",
    "    splits.append((train_data, val_data ,test_data))\n",
    "print(\"number of splits: \", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "tf.random.set_seed(1234)\n",
    "def build_model(input_steps, feature_count):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_steps, feature_count)),\n",
    "        tf.keras.layers.LSTM(128, activation='tanh', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)  #predicting one timestep\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d35f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data split\n",
    "\n",
    "#storage for training, validation, and test data\n",
    "all_train_X, all_train_X_indices, all_train_y, all_train_y_indices = [], [], [], []\n",
    "all_val_X, all_val_X_indices, all_val_y, all_val_y_indices = [], [], [], []\n",
    "all_test_X, all_test_X_indices, all_test_y, all_test_y_indices = [], [], [], []\n",
    "\n",
    "# loop over all folds to collect training data\n",
    "fold = 0\n",
    "for fold, (train_data, val_data, test_data) in enumerate(splits):\n",
    "    print(f\"Processing Fold {fold + 1}\")\n",
    "\n",
    "    #generate sliding windows for training\n",
    "    df_train_X, df_train_y, df_train_X_indices, df_train_y_indices = generate_sliding_window(train_data, input_steps, output_steps, features, target, step_size_train)\n",
    "    \n",
    "    #generate sliding windows for validation\n",
    "    df_val_X, df_val_y, df_val_X_indices, df_val_y_indices = generate_sliding_window(val_data, input_steps, output_steps, features, target, step_size_val)\n",
    "    \n",
    "    #generate sliding windows for testing\n",
    "    df_test_X, df_test_y, df_test_X_indices, df_test_y_indices = generate_sliding_window(test_data, input_steps, output_steps, features, target, step_size_test)\n",
    "\n",
    "    #append training data\n",
    "    all_train_X.extend(df[features].values for df in df_train_X)\n",
    "    all_train_X_indices.extend(df.values for df in df_train_X_indices)\n",
    "    all_train_y.extend(df.values for df in df_train_y)\n",
    "    all_train_y_indices.extend(df.values for df in df_train_y_indices)\n",
    "\n",
    "    #append validation data\n",
    "    all_val_X.extend(df[features].values for df in df_val_X)\n",
    "    all_val_X_indices.extend(df.values for df in df_val_X_indices)\n",
    "    all_val_y.extend(df.values for df in df_val_y)\n",
    "    all_val_y_indices.extend(df.values for df in df_val_y_indices)\n",
    "\n",
    "    #append test data\n",
    "    all_test_X.extend(df[features].values for df in df_test_X)\n",
    "    all_test_X_indices.extend(df.values for df in df_test_X_indices)\n",
    "    all_test_y.extend(df.values for df in df_test_y)\n",
    "    all_test_y_indices.extend(df.values for df in df_test_y_indices)\n",
    "\n",
    "#convert lists to numpy arrays\n",
    "all_train_X, all_train_y = np.array(all_train_X), np.array(all_train_y)\n",
    "all_train_X_indices, all_train_y_indices = np.array(all_train_X_indices), np.array(all_train_y_indices)\n",
    "\n",
    "all_val_X, all_val_y = np.array(all_val_X), np.array(all_val_y)\n",
    "all_val_X_indices, all_val_y_indices = np.array(all_val_X_indices), np.array(all_val_y_indices)\n",
    "\n",
    "all_test_X, all_test_y = np.array(all_test_X), np.array(all_test_y)\n",
    "all_test_X_indices, all_test_y_indices = np.array(all_test_X_indices), np.array(all_test_y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66705461",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_X = all_train_X.astype(np.float32)\n",
    "all_train_y = all_train_y.astype(np.float32)\n",
    "\n",
    "all_val_X = all_val_X.astype(np.float32)\n",
    "all_val_y = all_val_y.astype(np.float32)\n",
    "\n",
    "all_test_X = all_test_X.astype(np.float32)\n",
    "all_test_y = all_test_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0170d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(all_train_X).sum())  # Count NaNs\n",
    "print(np.isinf(all_train_X).sum())  # Count Infs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65d706",
   "metadata": {},
   "source": [
    "# benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a dictionary to store benchmark results\n",
    "benchmark_results = {}\n",
    "\n",
    "#function to compute and store evaluation metrics\n",
    "def evaluate_benchmark(name, y_true, y_pred):\n",
    "    \"\"\"Computes MAE, RMSE, MAPE, and R² and stores in a dictionary.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    #avoid zero division in MAPE\n",
    "    valid_mask = y_true != 0  \n",
    "    mape = np.mean(np.abs((y_true[valid_mask] - y_pred[valid_mask]) / y_true[valid_mask])) * 100  \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    #store results in dictionary\n",
    "    benchmark_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "    #print results\n",
    "    print(f\"  Benchmark: {name}\")\n",
    "    print(f\"  Test MAPE: {mape:.2f}%\")\n",
    "    print(f\"  Test R²: {r2:.4f}\")\n",
    "    print(f\"  Test MAE: {mae:.4f}\")\n",
    "    print(f\"  Test RMSE: {rmse:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91906862",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_y_flat = all_test_y.reshape(-1)  #ensure it's a 1D array\n",
    "all_test_y_indices_flat = all_test_y_indices.reshape(-1)  #flatten indices to match\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"actual\": all_test_y_flat,\n",
    "}, index=pd.Index(all_test_y_indices_flat, name=\"timestamp\"))\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark 1: the ideal profile\n",
    "#shift the 'adjusted_P_max' column forward by one day\n",
    "df_shifted = df_data[['adjusted_P_max']].shift(freq='D')\n",
    "\n",
    "df_shifted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark1 = df_test\n",
    "\n",
    "\n",
    "df_benchmark1 = df_benchmark1.reset_index()\n",
    "df_benchmark1['timestamp'] = pd.to_datetime(df_benchmark1['timestamp'])\n",
    "df_benchmark1['timestamp'] = df_benchmark1['timestamp'].dt.tz_localize('UTC')\n",
    "df_benchmark1.set_index('timestamp', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_benchmark1 = df_benchmark1.merge(df_shifted[['adjusted_P_max']], left_index=True, right_index=True, how='left')\n",
    "df_benchmark1 = df_benchmark1.merge(df_data[['mean_actualPowerTot_W_inverter']], left_index=True, right_index=True, how='left')\n",
    "df_benchmark1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182bcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_benchmark1[['mean_actualPowerTot_W_inverter', 'adjusted_P_max']].isna().sum())\n",
    "print(df_benchmark1[df_benchmark1['adjusted_P_max'].isna()])\n",
    "#hour change for summer hour, skip's an hour --> nan's\n",
    "df_benchmark1['adjusted_P_max'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_benchmark(\"15min_ideal_profile\", df_benchmark1['mean_actualPowerTot_W_inverter'] ,df_benchmark1['adjusted_P_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_benchmark1 = go.Figure()\n",
    "\n",
    "fig_benchmark1.add_trace(go.Scatter(x=df_benchmark1.index, y=df_benchmark1['adjusted_P_max'], mode='lines', name='prediction'))\n",
    "fig_benchmark1.add_trace(go.Scatter(x=df_benchmark1.index, y=df_benchmark1['mean_actualPowerTot_W_inverter'], mode='lines', name='actual'))\n",
    "\n",
    "#update layout for better visualization\n",
    "fig_benchmark1.update_layout(\n",
    "    title='benchmark 1',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Mean Actual Power (W)',\n",
    "    #xaxis_rangeslider_visible=True,\n",
    "    margin=dict(t=150),  # Increase top margin to fit legend and title\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # horizontal layout\n",
    "        y=1,           # place it above the plot area\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        range=['2022-05-05', '2022-05-10']\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    )\n",
    ")\n",
    "\n",
    "# show the plot\n",
    "fig_benchmark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark 2: 1 day shift\n",
    "df_shifted_day = df_data[['mean_actualPowerTot_W_inverter']].shift(freq='D')\n",
    "df_shifted_day = df_shifted_day.rename(columns={'mean_actualPowerTot_W_inverter': 'mean_actualPowerTot_W_inverter_shifted_1D'})\n",
    "df_shifted_day                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark2 = df_test\n",
    "\n",
    "\n",
    "df_benchmark2 = df_benchmark2.reset_index()\n",
    "df_benchmark2['timestamp'] = pd.to_datetime(df_benchmark2['timestamp'])\n",
    "df_benchmark2['timestamp'] = df_benchmark2['timestamp'].dt.tz_localize('UTC')\n",
    "df_benchmark2.set_index('timestamp', inplace=True)\n",
    "\n",
    "\n",
    "df_benchmark2 = df_benchmark2.merge(df_shifted_day[['mean_actualPowerTot_W_inverter_shifted_1D']], left_index=True, right_index=True, how='left')\n",
    "df_benchmark2 = df_benchmark2.merge(df_data[['mean_actualPowerTot_W_inverter']], left_index=True, right_index=True, how='left')\n",
    "df_benchmark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_benchmark2[['mean_actualPowerTot_W_inverter_shifted_1D', 'mean_actualPowerTot_W_inverter']].isna().sum())\n",
    "print(df_benchmark2[df_benchmark2['mean_actualPowerTot_W_inverter_shifted_1D'].isna()])\n",
    "#hour change for summer hour, skip's an hour --> nan's\n",
    "df_benchmark2['mean_actualPowerTot_W_inverter_shifted_1D'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_benchmark(\"15min_1day_shift\", df_benchmark2['mean_actualPowerTot_W_inverter'] ,df_benchmark2['mean_actualPowerTot_W_inverter_shifted_1D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_benchmark2 = go.Figure()\n",
    "\n",
    "fig_benchmark2.add_trace(go.Scatter(x=df_benchmark2.index, y=df_benchmark2['mean_actualPowerTot_W_inverter_shifted_1D'], mode='lines', name='prediction'))\n",
    "fig_benchmark2.add_trace(go.Scatter(x=df_benchmark2.index, y=df_benchmark2['mean_actualPowerTot_W_inverter'], mode='lines', name='actual'))\n",
    "\n",
    "#update layout for better visualization\n",
    "fig_benchmark2.update_layout(\n",
    "    title='benchmark 2',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Mean Actual Power (W)',\n",
    "    #xaxis_rangeslider_visible=True,\n",
    "    margin=dict(t=150),  # Increase top margin to fit legend and title\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # horizontal layout\n",
    "        y=1,           # place it above the plot area\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        range=['2022-05-05', '2022-05-10']\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_benchmark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark 3: autoregressive\n",
    "all_train_y_flat = all_train_y.reshape(-1)  #ensure it's a 1D array\n",
    "all_train_y_indices_flat = all_train_y_indices.reshape(-1)  #flatten indices to match\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    \"actual\": all_train_y_flat,\n",
    "}, index=pd.Index(all_train_y_indices_flat, name=\"timestamp\"))\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark 3: autoregressief model\n",
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "input_steps = 4*4\n",
    "\n",
    "#prepare training data\n",
    "train_series = df_train['actual']\n",
    "test_series = df_test['actual']\n",
    "\n",
    "#fit an autoregressive model\n",
    "model = AutoReg(train_series, lags=input_steps, old_names=False)\n",
    "model_fit = model.fit()\n",
    "\n",
    "#print model summary\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of hours to predict ahead\n",
    "prediction_horizon = 24  #number of hours to predict ahead\n",
    "\n",
    "#initialize lists to store predictions for the whole test set\n",
    "all_predictions_recursive = []\n",
    "all_predictions_indices = []\n",
    "\n",
    "#loop over the entire test set\n",
    "amount_of_times = 0\n",
    "for test_idx in range(0, len(all_test_X), 24*4):  #iterate over each test example\n",
    "    starting_window = all_test_X[test_idx]\n",
    "    starting_window_indeces = all_test_X_indices[test_idx]\n",
    "    #print(starting_window_indeces)\n",
    "\n",
    "    input_window = starting_window\n",
    "    input_window_indices = starting_window_indeces\n",
    "\n",
    "    predictions_recursive = []\n",
    "    predictions_indices = []\n",
    "    sin_recursive = []\n",
    "    cos_recursive = []\n",
    "\n",
    "    for i in range(prediction_horizon * 4):  #total steps in 15minutes intervals\n",
    "        #predict next minute\n",
    "        prediction = model_fit.predict(start=len(input_window), end=len(input_window), dynamic=False)\n",
    "        predictions_recursive.append(prediction[0])\n",
    "\n",
    "        #update time indices for predictions\n",
    "        next_index = input_window_indices[-1] + pd.Timedelta(minutes=15)\n",
    "        predictions_indices.append(next_index)\n",
    "\n",
    "        #update the input window for the next step (shift window to the left)\n",
    "        input_window = np.roll(input_window, -1, axis=0)\n",
    "        \n",
    "        #update the last element of the window with the prediction\n",
    "        input_window[-1, 0] = prediction[0]\n",
    "\n",
    "        #update hour sin and cos\n",
    "        next_hour = next_index.hour\n",
    "        input_window[-1, 1] = np.sin(2 * np.pi * next_hour / 24)\n",
    "        input_window[-1, 2] = np.cos(2 * np.pi * next_hour / 24)\n",
    "\n",
    "        #append sin and cos values\n",
    "        sin_recursive.append(input_window[-1, 1])\n",
    "        cos_recursive.append(input_window[-1, 2])\n",
    "\n",
    "        #update indices\n",
    "        input_window_indices = np.roll(input_window_indices, -1, axis=0)\n",
    "        input_window_indices[-1] = next_index\n",
    "\n",
    "        # print progress for every 100 steps or at the last step\n",
    "        if i % 10 == 0 or i == prediction_horizon * 4 - 1:\n",
    "            print(f\"Prediction progress for test set {test_idx + 1}: Step {i + 1} / {prediction_horizon * 4} ({(i + 1) / (prediction_horizon * 4) * 100:.2f}%)\")\n",
    "\n",
    "        \n",
    "    #convert predictions and indices to numpy arrays\n",
    "    predictions_recursive = np.array(predictions_recursive)\n",
    "    predictions_indices = np.array(predictions_indices)\n",
    "\n",
    "    #store predictions for this test example\n",
    "    all_predictions_recursive.append(predictions_recursive)\n",
    "    all_predictions_indices.append(predictions_indices)\n",
    "    \n",
    "    amount_of_times += 1\n",
    "    \n",
    "    #if(amount_of_times >= 10):\n",
    "    #    break\n",
    "    \n",
    "    \n",
    "#convert all predictions to arrays for easier handling\n",
    "all_predictions_recursive = np.array(all_predictions_recursive)\n",
    "all_predictions_indices = np.array(all_predictions_indices)\n",
    "\n",
    "print(\"Prediction completed for all test samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_all = pd.DataFrame({\n",
    "    \"predicted\": all_predictions_recursive.reshape(-1)\n",
    "}, index=pd.Index(all_predictions_indices.reshape(-1), name=\"timestamp\"))\n",
    "\n",
    "df_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925492c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring back to normal data\n",
    "#read in the normalization profile factors\n",
    "\n",
    "df_results_all = df_results_all.reset_index()\n",
    "df_results_all['timestamp'] = pd.to_datetime(df_results_all['timestamp'])\n",
    "df_results_all['timestamp'] = df_results_all['timestamp'].dt.tz_localize('UTC')\n",
    "df_results_all.set_index('timestamp', inplace=True)\n",
    "\n",
    "\n",
    "#merge the two DataFrames on the 'time' index\n",
    "df_merged_all = df_results_all.merge(df_data[['adjusted_P_max', 'mean_actualPowerTot_W_inverter']], left_index=True, right_index=True, how='left')\n",
    "#lose some data from full_adjusted_df near end since the df_data doesn't have full last day\n",
    "\n",
    "#check for missing values (NaN) in adjusted_P_max\n",
    "if df_merged_all['adjusted_P_max'].isna().any():\n",
    "    print(\"Warning: Some values are missing in the normalization profile.\")\n",
    "    \n",
    "    \n",
    "# currently cut of a part that goes infinite\n",
    "\n",
    "df_merged_all = df_merged_all[(df_merged_all[\"predicted\"] > 0) & (df_merged_all[\"predicted\"] < 5)]\n",
    "\n",
    "    \n",
    "#denormalize the 'mean_actualPowerTot_W_inverter' column by multiplying by the 'adjusted_P_max' column\n",
    "df_merged_all['denormalized_value_predicted'] = df_merged_all['predicted'] * df_merged_all['adjusted_P_max']\n",
    "\n",
    "\n",
    "df_merged_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947aa27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_all = df_merged_all.dropna(subset=['adjusted_P_max'])\n",
    "\n",
    "#verify that there are no more NaNs\n",
    "nan_count_after_drop = df_merged_all['adjusted_P_max'].isna().sum()\n",
    "print(f\"Number of NaN values in 'adjusted_P_max' after drop: {nan_count_after_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ebe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_all_denormalised = go.Figure()\n",
    "\n",
    "fig_all_denormalised.add_trace(go.Scatter(x=df_merged_all.index, y=df_merged_all['denormalized_value_predicted'], mode='lines', name='predicted'))\n",
    "fig_all_denormalised.add_trace(go.Scatter(x=df_merged_all.index, y=df_merged_all['mean_actualPowerTot_W_inverter'], mode='lines', name='actual'))\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig_all_denormalised.update_layout(\n",
    "    title='denormalised',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Mean Actual Power (W)',\n",
    "    #xaxis_rangeslider_visible=True,\n",
    "    margin=dict(t=150),  # Increase top margin to fit legend and title\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # horizontal layout\n",
    "        y=1,           # place it above the plot area\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        range=['2022-05-05', '2022-05-10']\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_all_denormalised.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_benchmark(\"15min_autoregressive\", df_merged_all['mean_actualPowerTot_W_inverter'] ,df_merged_all['denormalized_value_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3dd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7a9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e02be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute evaluation metrics\n",
    "def evaluate_benchmark2(y_true, y_pred):\n",
    "    \"\"\"Computes MAE, RMSE, MAPE, and R² and stores in a dictionary.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    #avoid zero division in MAPE\n",
    "    valid_mask = y_true != 0  \n",
    "    mape = np.mean(np.abs((y_true[valid_mask] - y_pred[valid_mask]) / y_true[valid_mask])) * 100  \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    results = {\n",
    "        \"MAPE\": round(mape, 2),\n",
    "        \"R²\": round(r2, 4),\n",
    "        \"MAE\": round(mae, 4),\n",
    "        \"RMSE\": round(rmse, 4),\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark 4: average month production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069cce72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark_results = {}\n",
    "\n",
    "for days in range(1, 31):\n",
    "    #create an empty DataFrame to hold the rolling timestamp-specific average\n",
    "    df_avg_shifted = df_data[['mean_actualPowerTot_W_inverter']].copy()\n",
    "\n",
    "    #initialize a Series for the 7-day timestamp-specific average\n",
    "    avg_values = []\n",
    "\n",
    "    #loop through each timestamp in df_data to compute average of the same time over the past 7 days\n",
    "    for current_time in df_avg_shifted.index:\n",
    "        #create list of 7 previous days at the same timestamp\n",
    "        past_times = [current_time - pd.Timedelta(days=day) for day in range(1, days + 1)]\n",
    "\n",
    "        #extract the 7 values from those past timestamps\n",
    "        past_values = [df_avg_shifted.loc[time, 'mean_actualPowerTot_W_inverter']\n",
    "                       if time in df_avg_shifted.index else np.nan\n",
    "                       for time in past_times]\n",
    "\n",
    "        #compute the average\n",
    "        avg_value = np.nanmean(past_values)\n",
    "\n",
    "        avg_values.append(avg_value)\n",
    "\n",
    "    #assign the computed averages\n",
    "    df_avg_shifted['mean_actualPowerTot_W_inverter_7D_timestamp_avg'] = avg_values\n",
    "\n",
    "    #shift the entire series forward by 1 day to simulate prediction for next day\n",
    "    #df_avg_shifted['mean_actualPowerTot_W_inverter_7D_timestamp_avg_shifted'] = (\n",
    "    #    df_avg_shifted['mean_actualPowerTot_W_inverter_7D_timestamp_avg'].shift(freq='D')\n",
    "    #)\n",
    "\n",
    "    #keep only the shifted column\n",
    "    df_shifted_7d_avg = df_avg_shifted[['mean_actualPowerTot_W_inverter_7D_timestamp_avg']]\n",
    "\n",
    "    #merge with test data\n",
    "    df_benchmark4 = df_test.copy()\n",
    "    df_benchmark4 = df_benchmark4.reset_index()\n",
    "    df_benchmark4['timestamp'] = pd.to_datetime(df_benchmark4['timestamp'])\n",
    "    df_benchmark4['timestamp'] = df_benchmark4['timestamp'].dt.tz_localize('UTC')\n",
    "    df_benchmark4.set_index('timestamp', inplace=True)\n",
    "\n",
    "    df_benchmark4 = df_benchmark4.merge(df_shifted_7d_avg, left_index=True, right_index=True, how='left')\n",
    "    df_benchmark4 = df_benchmark4.merge(df_data[['mean_actualPowerTot_W_inverter']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    #handle missing values (e.g., early data or daylight saving)\n",
    "    print(df_benchmark4[['mean_actualPowerTot_W_inverter_7D_timestamp_avg', 'mean_actualPowerTot_W_inverter']].isna().sum())\n",
    "    df_benchmark4['mean_actualPowerTot_W_inverter_7D_timestamp_avg'].fillna(0, inplace=True)\n",
    "\n",
    "    #evaluate benchmark\n",
    "    benchmark_results[days] = evaluate_benchmark2(df_benchmark4['mean_actualPowerTot_W_inverter'], \n",
    "                       df_benchmark4['mean_actualPowerTot_W_inverter_7D_timestamp_avg'])\n",
    "\n",
    "#output all the benchmark results for each days range (1 to 30)\n",
    "print(\"Benchmark results for 1 to 30 days averages:\")\n",
    "for days, result in benchmark_results.items():\n",
    "    print(f\"Days: {days}, Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_benchmark4 = go.Figure()\n",
    "\n",
    "fig_benchmark4.add_trace(go.Scatter(x=df_benchmark4.index, y=df_benchmark4['mean_actualPowerTot_W_inverter_7D_timestamp_avg'], mode='lines', name='prediction'))\n",
    "fig_benchmark4.add_trace(go.Scatter(x=df_benchmark4.index, y=df_benchmark4['mean_actualPowerTot_W_inverter'], mode='lines', name='actual'))\n",
    "\n",
    "#update layout for better visualization\n",
    "fig_benchmark4.update_layout(\n",
    "    title='benchmark 4',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Mean Actual Power (W)',\n",
    "    #xaxis_rangeslider_visible=True,\n",
    "    margin=dict(t=150),  # Increase top margin to fit legend and title\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # horizontal layout\n",
    "        y=1,           # place it above the plot area\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        range=['2022-05-05', '2022-05-10']\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    )\n",
    ")\n",
    "\n",
    "# show the plot\n",
    "fig_benchmark4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#extract the MAE values from the benchmark_results dictionary\n",
    "mae_values = []\n",
    "days_range = range(1, 31)\n",
    "\n",
    "for days in days_range:\n",
    "    result = benchmark_results.get(days)\n",
    "    if result:\n",
    "        mae = result.get('R²', None)\n",
    "        if mae is not None:\n",
    "            mae_values.append(mae)\n",
    "        else:\n",
    "            mae_values.append(np.nan)\n",
    "    else:\n",
    "        mae_values.append(np.nan)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(days_range, mae_values, marker='o', color='b', linestyle='-', label='MAE')\n",
    "\n",
    "#adding text labels to each point\n",
    "for i, txt in enumerate(mae_values):\n",
    "    if not np.isnan(txt):  #only label valid values\n",
    "        plt.text(days_range[i], txt, f\"{txt:.2f}\", fontsize=10, ha='right', va='bottom')\n",
    "\n",
    "plt.xlabel('Days Back (1 to 30)', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Percentage Error (MAPE)', fontsize=12)\n",
    "plt.title('R² for Different Lookback Periods (1 to 30 days)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
